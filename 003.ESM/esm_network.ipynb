{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRG5Y7-Yow-0"
      },
      "source": [
        "# Training the ESM Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GZccvN_Jow-8"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "# Imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
        "import torch.nn.functional as F  # All functions that don't have any parameters\n",
        "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_curve, matthews_corrcoef\n",
        "# from google.colab import drive\n",
        "import os\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import esm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPad9FUEp-Zo",
        "outputId": "94b586a3-b163-479e-da3a-49b800714701"
      },
      "outputs": [],
      "source": [
        "#drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RfcPziZ_qNU_"
      },
      "outputs": [],
      "source": [
        "#os.chdir(\"/content/drive/MyDrive/hackathon_data_scripts/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7y5mWeyow_B"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_mhc(data):\n",
        "    '''\n",
        "    Removes the MHC molecule, and sets the transformer terms to the new first residues\n",
        "    '''\n",
        "    global_terms = data[:,:3,54:]\n",
        "    data = data[:,179:,:]\n",
        "    data[:,:3,54:] = global_terms\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pvANGSmow_C",
        "outputId": "2b2947cf-84d2-4965-ca4b-b6622e9b6d27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val set shape: 1526 241 1334\n",
            "Percent positive samples in train: 25.0\n",
            "Percent positive samples in val: 24.901703800786372\n",
            "\n",
            "NOTE:\n",
            "Setting batch-size to 64\n",
            "Using device (CPU/GPU): cpu\n"
          ]
        }
      ],
      "source": [
        "X_train = np.load('../data/X_train_mean_emb.npz')['arr_0']\n",
        "X_train = remove_mhc(X_train)\n",
        "\n",
        "X_val = np.load('../data/X_val_mean_emb.npz')['arr_0']\n",
        "X_val = remove_mhc(X_val)\n",
        "\n",
        "y_train = np.load('../data/y_train.npz')['arr_0']\n",
        "y_val = np.load('../data/y_val.npz')['arr_0']\n",
        "\n",
        "nsamples, nx, ny = X_val.shape\n",
        "print(\"val set shape:\", nsamples, nx, ny)\n",
        "\n",
        "p_neg = len(y_train[y_train == 1]) / len(y_train) * 100\n",
        "print(\"Percent positive samples in train:\", p_neg)\n",
        "\n",
        "p_pos = len(y_val[y_val == 1]) / len(y_val) * 100\n",
        "print(\"Percent positive samples in val:\", p_pos)\n",
        "\n",
        "# make the data set into one dataset that can go into dataloader\n",
        "train_ds = []\n",
        "for i in range(len(X_train)):\n",
        "    train_ds.append([np.transpose(X_train[i]), y_train[i]])\n",
        "\n",
        "val_ds = []\n",
        "for i in range(len(X_val)):\n",
        "    val_ds.append([np.transpose(X_val[i]), y_val[i]])\n",
        "    \n",
        "bat_size = 64\n",
        "print(\"\\nNOTE:\\nSetting batch-size to\", bat_size)\n",
        "train_ldr = torch.utils.data.DataLoader(train_ds, batch_size=bat_size, shuffle=True)\n",
        "val_ldr = torch.utils.data.DataLoader(val_ds, batch_size=bat_size, shuffle=True)\n",
        "\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device (CPU/GPU):\", device)\n",
        "# device = torch.device(\"cpu\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss weight 0.25\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "# input_size = 420\n",
        "_, input_size, n_features = X_train.shape\n",
        "n_local_feat = 27\n",
        "n_global_feat = n_features - n_local_feat\n",
        "num_classes = 1\n",
        "# learning_rate = 0.01\n",
        "learning_rate = 0.001\n",
        "\n",
        "loss_weight = sum(y_train) / len(y_train)\n",
        "print(\"loss weight\", loss_weight)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self,  num_classes):\n",
        "        super(Net, self).__init__()   \n",
        "        self.bn0 = nn.BatchNorm1d(n_local_feat)\n",
        "        self.conv1 = nn.Conv1d(in_channels=n_local_feat, out_channels=100, kernel_size=3, stride=2, padding=1)\n",
        "        torch.nn.init.kaiming_uniform_(self.conv1.weight)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.conv1_bn = nn.BatchNorm1d(100)\n",
        "        \n",
        "        self.conv2 = nn.Conv1d(in_channels=100, out_channels=100, kernel_size=3, stride=2, padding=1)\n",
        "        torch.nn.init.kaiming_uniform_(self.conv2.weight)\n",
        "        self.conv2_bn = nn.BatchNorm1d(100)\n",
        "        \n",
        "        ######## code from master thesis \n",
        "        self.rnn = nn.LSTM(input_size=100,hidden_size=26,num_layers=3, dropout=0.5, batch_first=True, bidirectional = True)\n",
        "        self.drop = nn.Dropout(p = 0.5) # Dunno if dropout should be even higher?? - Christian\n",
        "        self.fc1 = nn.Linear(26*2 + n_global_feat, 26*2 + n_global_feat)\n",
        "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        ########\n",
        "        \n",
        "        # since we add new features in this step, we have to use batch normalization again\n",
        "        self.bn1 = nn.BatchNorm1d(26*2 + n_global_feat)\n",
        "        # if we pipe the global terms innto the fc, we should have more than just 1\n",
        "        self.fc2 = nn.Linear(26*2 + n_global_feat, num_classes)\n",
        "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        local_features = x[:, :27, :]\n",
        "        # global features are the same for the whole sequence -> take first value\n",
        "        global_features = x[:, 27:, 0]\n",
        "\n",
        "        ######## code from master thesis\n",
        "        x = self.bn0(local_features)\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.conv1_bn(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.conv2_bn(x)\n",
        "        x = self.drop(x)\n",
        "        x = x.transpose_(2, 1)\n",
        "        x, (h, c) = self.rnn(x)\n",
        "        # concatenate bidirectional output of last layer\n",
        "        cat = torch.cat((h[-2, :, :], h[-1, :, :]), dim=1)\n",
        "        # add global features\n",
        "        x = torch.cat((cat, global_features), dim=1)\n",
        "        x = self.drop(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.drop(x)\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        ########\n",
        "        \n",
        "        return x\n",
        "    \n",
        "# Initialize network\n",
        "net = Net(num_classes=num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCELoss(reduction='none')  # for weighted loss\n",
        "# optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate,\n",
        "    weight_decay=0.0005,\n",
        "    amsgrad=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-6874d224e75e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0membed_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1280\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0membed_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMHC_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmap_mhc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0membed_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpep_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmap_peptide\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0membed_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtcr_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmap_tcr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "num_epochs = 100\n",
        "\n",
        "train_acc, train_loss = [], []\n",
        "valid_acc, valid_loss = [], []\n",
        "losses = []\n",
        "val_losses = []\n",
        "\n",
        "# for early stopping\n",
        "no_epoch_improve = 0\n",
        "min_val_loss = np.Inf\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    cur_loss = 0\n",
        "    val_loss = 0\n",
        "\n",
        "    net.train()\n",
        "    train_preds, train_preds_auc, train_targs = [], [], []\n",
        "    for batch_idx, (data, target) in enumerate(train_ldr):\n",
        "        X_batch = data.float().detach().requires_grad_(True)\n",
        "        target_batch = torch.tensor(np.array(target), dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = net(X_batch)\n",
        "\n",
        "        # calculate weighted loss\n",
        "        intermediate_loss = criterion(output, target_batch)\n",
        "        weights = torch.FloatTensor(abs(target_batch - loss_weight))\n",
        "        batch_loss = torch.mean(weights * intermediate_loss)\n",
        "        # batch_loss = criterion(output, target_batch)\n",
        "        \n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = np.round(output.detach().cpu())\n",
        "        preds_auc = output.detach().cpu()\n",
        "        train_targs += list(np.array(target_batch.cpu()))\n",
        "        train_preds += list(preds.data.numpy().flatten())\n",
        "        train_preds_auc += list(preds_auc.data.numpy().flatten())\n",
        "        cur_loss += batch_loss.detach()\n",
        "\n",
        "    losses.append(cur_loss / len(train_ldr.dataset))\n",
        "\n",
        "    net.eval()\n",
        "    ### Evaluate validation\n",
        "    val_preds, val_preds_auc, val_targs = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(val_ldr):  ###\n",
        "            x_batch_val = data.float().detach()\n",
        "            y_batch_val = target.float().detach().unsqueeze(1)\n",
        "\n",
        "            output = net(x_batch_val)\n",
        "\n",
        "            # calculate weighted loss\n",
        "            intermediate_loss = criterion(output, y_batch_val)\n",
        "            weights = torch.FloatTensor(abs(y_batch_val - loss_weight))\n",
        "            val_batch_loss = torch.mean(weights * intermediate_loss)\n",
        "            # val_batch_loss = criterion(output, y_batch_val)\n",
        "\n",
        "            preds = np.round(output.detach())\n",
        "            val_preds += list(preds.data.numpy().flatten())\n",
        "            preds_auc = output.detach()\n",
        "            val_preds_auc += list(preds_auc.data.numpy().flatten())\n",
        "            val_targs += list(np.array(y_batch_val))\n",
        "            val_loss += val_batch_loss.detach()\n",
        "\n",
        "        val_losses.append(val_loss / len(val_ldr.dataset))\n",
        "        print(\"\\nEpoch:\", epoch + 1)\n",
        "\n",
        "        train_acc_cur = accuracy_score(train_targs, train_preds)\n",
        "        valid_acc_cur = accuracy_score(val_targs, val_preds)\n",
        "\n",
        "        train_acc.append(train_acc_cur)\n",
        "        valid_acc.append(valid_acc_cur)\n",
        "\n",
        "\n",
        "        print(\n",
        "            \"Training loss:\",\n",
        "            losses[-1].item(),\n",
        "            \"Validation loss:\",\n",
        "            val_losses[-1].item(),\n",
        "            end=\"\\n\",\n",
        "        )\n",
        "        print(\n",
        "            \"MCC Train:\",\n",
        "            matthews_corrcoef(train_targs, train_preds),\n",
        "            \"MCC val:\",\n",
        "            matthews_corrcoef(val_targs, val_preds),\n",
        "        )\n",
        "        \n",
        "    # Early stopping: no improvement in validation loss in 10 consecutive epochs\n",
        "    if (val_loss / len(X_val)).item() < min_val_loss:\n",
        "        no_epoch_improve = 0\n",
        "        min_val_loss = (val_loss / len(X_val))\n",
        "        torch.save(net, 'early_stopping_model.pt')\n",
        "        best_epoch = epoch\n",
        "    else:\n",
        "        no_epoch_improve +=1\n",
        "    if no_epoch_improve == 10:\n",
        "        print(\"Early stopping\\n\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Plots of training epochs\n",
        "epoch = np.arange(1, len(train_acc) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epoch, losses, \"r\", epoch, val_losses, \"b\")\n",
        "plt.legend([\"Train Loss\", \"Validation Loss\"])\n",
        "plt.vlines(best_epoch, ymin=0, ymax=0.005, linestyles='dashed')\n",
        "plt.xlabel(\"Epoch\"), plt.ylabel(\"Loss\")\n",
        "\n",
        "epoch = np.arange(1, len(train_acc) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epoch, train_acc, \"r\", epoch, valid_acc, \"b\")\n",
        "plt.legend([\"Train Accuracy\", \"Validation Accuracy\"])\n",
        "plt.vlines(best_epoch, ymin=0, ymax=0.9, linestyles='dashed')\n",
        "plt.xlabel(\"Epoch\"), plt.ylabel(\"Acc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance evaluation metrics of final model\n",
        "\n",
        "final_model = torch.load('early_stopping_model.pt')\n",
        "final_model.train()\n",
        "train_preds, train_preds_auc, train_targs = [], [], []\n",
        "for batch_idx, (data, target) in enumerate(train_ldr):\n",
        "    X_batch = data.float().detach().requires_grad_(True)\n",
        "    target_batch = torch.tensor(np.array(target), dtype=torch.float).unsqueeze(1)\n",
        "    \n",
        "    output = final_model(X_batch)\n",
        "    preds = np.round(output.detach().cpu())\n",
        "    preds_auc = output.detach().cpu()\n",
        "    train_targs += list(np.array(target_batch.cpu()))\n",
        "    train_preds += list(preds.data.numpy().flatten())\n",
        "    train_preds_auc += list(preds_auc.data.numpy().flatten())\n",
        "\n",
        "final_model.eval()\n",
        "val_preds, val_preds_auc, val_targs = [], [], []\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (data, target) in enumerate(val_ldr):  ###\n",
        "        x_batch_val = data.float().detach()\n",
        "        y_batch_val = target.float().detach().unsqueeze(1)\n",
        "\n",
        "        output = final_model(x_batch_val)\n",
        "\n",
        "        preds = np.round(output.detach())\n",
        "        val_preds += list(preds.data.numpy().flatten())\n",
        "        preds_auc = output.detach()\n",
        "        val_preds_auc += list(preds_auc.data.numpy().flatten())\n",
        "        val_targs += list(np.array(y_batch_val))\n",
        "        val_loss += val_batch_loss.detach()\n",
        "\n",
        "print(\"MCC Train:\", matthews_corrcoef(train_targs, train_preds))\n",
        "print(\"MCC Test:\", matthews_corrcoef(val_targs, val_preds))\n",
        "\n",
        "prec_val = metrics.precision_score(val_targs, val_preds)\n",
        "rec_val = metrics.recall_score(val_targs, val_preds)\n",
        "f1_val = 2 * ((prec_val * rec_val) / (prec_val + rec_val))\n",
        "\n",
        "print(\"Precision Test:\", prec_val)\n",
        "print(\"Recall Test:\", rec_val)\n",
        "print(\"F1 Test:\", f1_val)\n",
        "\n",
        "print(\"Confusion matrix train:\", confusion_matrix(train_targs, train_preds), sep=\"\\n\")\n",
        "print(\"Confusion matrix test:\", confusion_matrix(val_targs, val_preds), sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_roc(targets, predictions):\n",
        "    # ROC\n",
        "    fpr, tpr, threshold = metrics.roc_curve(targets, predictions)\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "    # plot ROC\n",
        "    plt.figure()\n",
        "    plt.title(\"Receiver Operating Characteristic\")\n",
        "    plt.plot(fpr, tpr, \"b\", label=\"AUC = %0.2f\" % roc_auc)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.plot([0, 1], [0, 1], \"r--\")\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    # plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_roc(train_targs, train_preds_auc)\n",
        "plt.title(\"Training AUC\")\n",
        "plot_roc(val_targs, val_preds_auc)\n",
        "plt.title(\"Validation AUC\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Helpful scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "AnTG9qjdow_T",
        "outputId": "4819cab0-f8fe-4e56-95bc-97eaada88215"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.read_csv(\"../hackathon_data_scripts/data/example.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "LQhXwSnPow_U",
        "outputId": "ed2a3533-6b31-4f29-d1ca-9b2f70c95bc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Showing first complex as dataframe. Columns are positions and indices are calculated features\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>C</th>\n",
              "      <th>D</th>\n",
              "      <th>E</th>\n",
              "      <th>F</th>\n",
              "      <th>G</th>\n",
              "      <th>H</th>\n",
              "      <th>I</th>\n",
              "      <th>K</th>\n",
              "      <th>L</th>\n",
              "      <th>M</th>\n",
              "      <th>N</th>\n",
              "      <th>P</th>\n",
              "      <th>Q</th>\n",
              "      <th>R</th>\n",
              "      <th>S</th>\n",
              "      <th>T</th>\n",
              "      <th>V</th>\n",
              "      <th>W</th>\n",
              "      <th>Y</th>\n",
              "      <th>per_res_fa_atr</th>\n",
              "      <th>per_res_fa_rep</th>\n",
              "      <th>per_res_fa_sol</th>\n",
              "      <th>per_res_fa_elec</th>\n",
              "      <th>per_res_fa_dun</th>\n",
              "      <th>per_res_p_aa_pp</th>\n",
              "      <th>per_res_score</th>\n",
              "      <th>foldx_MP</th>\n",
              "      <th>foldx_MA</th>\n",
              "      <th>foldx_MB</th>\n",
              "      <th>foldx_PA</th>\n",
              "      <th>foldx_PB</th>\n",
              "      <th>foldx_AB</th>\n",
              "      <th>global_complex_total_score</th>\n",
              "      <th>global_complex_fa_atr</th>\n",
              "      <th>global_complex_fa_dun</th>\n",
              "      <th>global_complex_fa_elec</th>\n",
              "      <th>global_complex_fa_rep</th>\n",
              "      <th>global_complex_fa_sol</th>\n",
              "      <th>global_complex_p_aa_pp</th>\n",
              "      <th>global_tcr_total_score</th>\n",
              "      <th>global_tcr_fa_atr</th>\n",
              "      <th>global_tcr_fa_dun</th>\n",
              "      <th>global_tcr_fa_elec</th>\n",
              "      <th>global_tcr_fa_rep</th>\n",
              "      <th>global_tcr_fa_sol</th>\n",
              "      <th>global_tcr_p_aa_pp</th>\n",
              "      <th>global_pmhc_total_score</th>\n",
              "      <th>global_pmhc_fa_atr</th>\n",
              "      <th>global_pmhc_fa_dun</th>\n",
              "      <th>global_pmhc_fa_elec</th>\n",
              "      <th>global_pmhc_fa_rep</th>\n",
              "      <th>global_pmhc_fa_sol</th>\n",
              "      <th>global_pmhc_p_aa_pp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.691</td>\n",
              "      <td>0.095</td>\n",
              "      <td>1.968</td>\n",
              "      <td>-0.287</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.833</td>\n",
              "      <td>-5.31471</td>\n",
              "      <td>-0.01875</td>\n",
              "      <td>-1.57906</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.11466</td>\n",
              "      <td>-1.22179</td>\n",
              "      <td>-1198.017</td>\n",
              "      <td>489.407</td>\n",
              "      <td>-716.398</td>\n",
              "      <td>4.547</td>\n",
              "      <td>1444.857</td>\n",
              "      <td>-87.484</td>\n",
              "      <td>2.265</td>\n",
              "      <td>-630.983</td>\n",
              "      <td>252.669</td>\n",
              "      <td>-355.891</td>\n",
              "      <td>2.352</td>\n",
              "      <td>680.346</td>\n",
              "      <td>-49.781</td>\n",
              "      <td>2.017</td>\n",
              "      <td>-564.837</td>\n",
              "      <td>235.9</td>\n",
              "      <td>-369.03</td>\n",
              "      <td>2.223</td>\n",
              "      <td>730.223</td>\n",
              "      <td>-36.813</td>\n",
              "      <td>0.562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.559</td>\n",
              "      <td>0.106</td>\n",
              "      <td>1.995</td>\n",
              "      <td>-0.806</td>\n",
              "      <td>0.077</td>\n",
              "      <td>-0.431</td>\n",
              "      <td>-2.419</td>\n",
              "      <td>-5.31471</td>\n",
              "      <td>-0.01875</td>\n",
              "      <td>-1.57906</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.11466</td>\n",
              "      <td>-1.22179</td>\n",
              "      <td>-1198.017</td>\n",
              "      <td>489.407</td>\n",
              "      <td>-716.398</td>\n",
              "      <td>4.547</td>\n",
              "      <td>1444.857</td>\n",
              "      <td>-87.484</td>\n",
              "      <td>2.265</td>\n",
              "      <td>-630.983</td>\n",
              "      <td>252.669</td>\n",
              "      <td>-355.891</td>\n",
              "      <td>2.352</td>\n",
              "      <td>680.346</td>\n",
              "      <td>-49.781</td>\n",
              "      <td>2.017</td>\n",
              "      <td>-564.837</td>\n",
              "      <td>235.9</td>\n",
              "      <td>-369.03</td>\n",
              "      <td>2.223</td>\n",
              "      <td>730.223</td>\n",
              "      <td>-36.813</td>\n",
              "      <td>0.562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-7.444</td>\n",
              "      <td>0.565</td>\n",
              "      <td>5.379</td>\n",
              "      <td>-3.654</td>\n",
              "      <td>1.607</td>\n",
              "      <td>-0.254</td>\n",
              "      <td>-6.032</td>\n",
              "      <td>-5.31471</td>\n",
              "      <td>-0.01875</td>\n",
              "      <td>-1.57906</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.11466</td>\n",
              "      <td>-1.22179</td>\n",
              "      <td>-1198.017</td>\n",
              "      <td>489.407</td>\n",
              "      <td>-716.398</td>\n",
              "      <td>4.547</td>\n",
              "      <td>1444.857</td>\n",
              "      <td>-87.484</td>\n",
              "      <td>2.265</td>\n",
              "      <td>-630.983</td>\n",
              "      <td>252.669</td>\n",
              "      <td>-355.891</td>\n",
              "      <td>2.352</td>\n",
              "      <td>680.346</td>\n",
              "      <td>-49.781</td>\n",
              "      <td>2.017</td>\n",
              "      <td>-564.837</td>\n",
              "      <td>235.9</td>\n",
              "      <td>-369.03</td>\n",
              "      <td>2.223</td>\n",
              "      <td>730.223</td>\n",
              "      <td>-36.813</td>\n",
              "      <td>0.562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-4.058</td>\n",
              "      <td>0.197</td>\n",
              "      <td>3.959</td>\n",
              "      <td>-2.659</td>\n",
              "      <td>0.216</td>\n",
              "      <td>-0.488</td>\n",
              "      <td>-4.343</td>\n",
              "      <td>-5.31471</td>\n",
              "      <td>-0.01875</td>\n",
              "      <td>-1.57906</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.11466</td>\n",
              "      <td>-1.22179</td>\n",
              "      <td>-1198.017</td>\n",
              "      <td>489.407</td>\n",
              "      <td>-716.398</td>\n",
              "      <td>4.547</td>\n",
              "      <td>1444.857</td>\n",
              "      <td>-87.484</td>\n",
              "      <td>2.265</td>\n",
              "      <td>-630.983</td>\n",
              "      <td>252.669</td>\n",
              "      <td>-355.891</td>\n",
              "      <td>2.352</td>\n",
              "      <td>680.346</td>\n",
              "      <td>-49.781</td>\n",
              "      <td>2.017</td>\n",
              "      <td>-564.837</td>\n",
              "      <td>235.9</td>\n",
              "      <td>-369.03</td>\n",
              "      <td>2.223</td>\n",
              "      <td>730.223</td>\n",
              "      <td>-36.813</td>\n",
              "      <td>0.562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-9.072</td>\n",
              "      <td>1.205</td>\n",
              "      <td>2.808</td>\n",
              "      <td>-1.844</td>\n",
              "      <td>2.182</td>\n",
              "      <td>0.060</td>\n",
              "      <td>-4.166</td>\n",
              "      <td>-5.31471</td>\n",
              "      <td>-0.01875</td>\n",
              "      <td>-1.57906</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.11466</td>\n",
              "      <td>-1.22179</td>\n",
              "      <td>-1198.017</td>\n",
              "      <td>489.407</td>\n",
              "      <td>-716.398</td>\n",
              "      <td>4.547</td>\n",
              "      <td>1444.857</td>\n",
              "      <td>-87.484</td>\n",
              "      <td>2.265</td>\n",
              "      <td>-630.983</td>\n",
              "      <td>252.669</td>\n",
              "      <td>-355.891</td>\n",
              "      <td>2.352</td>\n",
              "      <td>680.346</td>\n",
              "      <td>-49.781</td>\n",
              "      <td>2.017</td>\n",
              "      <td>-564.837</td>\n",
              "      <td>235.9</td>\n",
              "      <td>-369.03</td>\n",
              "      <td>2.223</td>\n",
              "      <td>730.223</td>\n",
              "      <td>-36.813</td>\n",
              "      <td>0.562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>415</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>417</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>418</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>419</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>420 rows  54 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       A    C  ...   global_pmhc_fa_sol   global_pmhc_p_aa_pp\n",
              "0    0.0  0.0  ...              -36.813                 0.562\n",
              "1    0.0  0.0  ...              -36.813                 0.562\n",
              "2    0.0  0.0  ...              -36.813                 0.562\n",
              "3    0.0  0.0  ...              -36.813                 0.562\n",
              "4    0.0  0.0  ...              -36.813                 0.562\n",
              "..   ...  ...  ...                  ...                   ...\n",
              "415  0.0  0.0  ...                0.000                 0.000\n",
              "416  0.0  0.0  ...                0.000                 0.000\n",
              "417  0.0  0.0  ...                0.000                 0.000\n",
              "418  0.0  0.0  ...                0.000                 0.000\n",
              "419  0.0  0.0  ...                0.000                 0.000\n",
              "\n",
              "[420 rows x 54 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def copy_as_dataframes(dataset_X):\n",
        "    \"\"\"\n",
        "    Returns list of DataFrames with named features from dataset_X,\n",
        "    using example CSV file\n",
        "    \"\"\"\n",
        "    df_raw = pd.read_csv(\"../hackathon_data_scripts/data/example.csv\")\n",
        "    return [pd.DataFrame(arr, columns=df_raw.columns) for arr in dataset_X]\n",
        "\n",
        "\n",
        "named_dataframes = copy_as_dataframes(X_train)\n",
        "print(\n",
        "    \"Showing first complex as dataframe. Columns are positions and indices are calculated features\"\n",
        ")\n",
        "named_dataframes[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y8r_3ZUow_X"
      },
      "source": [
        "# View complex MHC, peptide and TCR alpha/beta sequences\n",
        "You may want to view the one-hot encoded sequences as sequences in single-letter amino-acid format. The below function will return the TCR, peptide and MHC sequences for the dataset as 3 lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uFsvT0bdow_X"
      },
      "outputs": [],
      "source": [
        "def oneHot(residue):\n",
        "    \"\"\"\n",
        "    Converts string sequence to one-hot encoding\n",
        "    Example usage:\n",
        "    seq = \"GSHSMRY\"\n",
        "    oneHot(seq)\n",
        "    \"\"\"\n",
        "\n",
        "    mapping = dict(zip(\"ACDEFGHIKLMNPQRSTVWY\", range(20)))\n",
        "    if residue in \"ACDEFGHIKLMNPQRSTVWY\":\n",
        "        return np.eye(20)[mapping[residue]]\n",
        "    else:\n",
        "        return np.zeros(20)\n",
        "\n",
        "\n",
        "def reverseOneHot(encoding):\n",
        "    \"\"\"\n",
        "    Converts one-hot encoded array back to string sequence\n",
        "    \"\"\"\n",
        "    mapping = dict(zip(range(20), \"ACDEFGHIKLMNPQRSTVWY\"))\n",
        "    seq = \"\"\n",
        "    for i in range(len(encoding)):\n",
        "        if np.max(encoding[i]) > 0:\n",
        "            seq += mapping[np.argmax(encoding[i])]\n",
        "    return seq\n",
        "\n",
        "\n",
        "def extract_sequences(dataset_X):\n",
        "    \"\"\"\n",
        "    Return DataFrame with MHC, peptide and TCR a/b sequences from\n",
        "    one-hot encoded complex sequences in dataset X\n",
        "    \"\"\"\n",
        "    mhc_sequences = [reverseOneHot(arr[0:179, 0:20]) for arr in dataset_X]\n",
        "    pep_sequences = [reverseOneHot(arr[179:190, 0:20]) for arr in dataset_X]\n",
        "    tcr_sequences = [reverseOneHot(arr[192:, 0:20]) for arr in dataset_X]\n",
        "    df_sequences = pd.DataFrame(\n",
        "        {\"MHC\": mhc_sequences, \"peptide\": pep_sequences, \"tcr\": tcr_sequences}\n",
        "    )\n",
        "    return df_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "jAy2xRJnow_Z",
        "outputId": "582047d5-c20f-4229-daaf-bd24dc9ab49c",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Showing MHC, peptide and TCR alpha/beta sequences for each complex\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MHC</th>\n",
              "      <th>peptide</th>\n",
              "      <th>tcr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRME...</td>\n",
              "      <td>GILGFVFTL</td>\n",
              "      <td>EQSPQFLSIQEGENLTVYCNSSSVFSSLQWYRQEPGEGPVLLVTVV...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRME...</td>\n",
              "      <td>GILGFVFTL</td>\n",
              "      <td>EQSPQFLSIQEGENLTVYCNSSSVFSSLQWYRQEPGEGPVLLVTVV...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRME...</td>\n",
              "      <td>GILGFVFTL</td>\n",
              "      <td>QQVKQNSPSLSVQEGRISILNCDYTNSMFDYFLWYKKYPAEGPTFL...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRME...</td>\n",
              "      <td>GILGFVFTL</td>\n",
              "      <td>EQSPQFLSIQEGENLTVYCNSSSVFSSLQWYRQEPGEGPVLLVTVV...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRME...</td>\n",
              "      <td>GILGFVFTL</td>\n",
              "      <td>EQSPQFLSIQEGENLTVYCNSSSVFSSLQWYRQEPGEGPVLLVTVV...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521</th>\n",
              "      <td>GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRME...</td>\n",
              "      <td>FLYALALLL</td>\n",
              "      <td>QSPQSMFIQEGEDVSMNCTSSSIFNTWLWYKQEPGEGPVLLIALYK...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1522</th>\n",
              "      <td>GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRME...</td>\n",
              "      <td>LLFGYPVYV</td>\n",
              "      <td>PQALSIQEGENATMNCSYKTSINNLQWYRQNSGRGLVHLILIRSNE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1523</th>\n",
              "      <td>GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRME...</td>\n",
              "      <td>GLCTLVAML</td>\n",
              "      <td>EQSPQFLSIQEGENLTVYCNSSSVFSSLQWYRQEPGEGPVLLVTVV...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1524</th>\n",
              "      <td>GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRME...</td>\n",
              "      <td>FLYALALLL</td>\n",
              "      <td>QQVKQNSPSLSVQEGRISILNCDYTNSMFDYFLWYKKYPAEGPTFL...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1525</th>\n",
              "      <td>GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRME...</td>\n",
              "      <td>GLCTLVAML</td>\n",
              "      <td>EQSPQSLIVQEGKNLTINCTSSKTLYGLYWYKQKYGEGLIFLMMLQ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1526 rows  3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    MHC    peptide  \\\n",
              "0     GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRME...  GILGFVFTL   \n",
              "1     GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRME...  GILGFVFTL   \n",
              "2     GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRME...  GILGFVFTL   \n",
              "3     GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRME...  GILGFVFTL   \n",
              "4     GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRME...  GILGFVFTL   \n",
              "...                                                 ...        ...   \n",
              "1521  GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRME...  FLYALALLL   \n",
              "1522  GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRME...  LLFGYPVYV   \n",
              "1523  GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRME...  GLCTLVAML   \n",
              "1524  GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRME...  FLYALALLL   \n",
              "1525  GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRME...  GLCTLVAML   \n",
              "\n",
              "                                                    tcr  \n",
              "0     EQSPQFLSIQEGENLTVYCNSSSVFSSLQWYRQEPGEGPVLLVTVV...  \n",
              "1     EQSPQFLSIQEGENLTVYCNSSSVFSSLQWYRQEPGEGPVLLVTVV...  \n",
              "2     QQVKQNSPSLSVQEGRISILNCDYTNSMFDYFLWYKKYPAEGPTFL...  \n",
              "3     EQSPQFLSIQEGENLTVYCNSSSVFSSLQWYRQEPGEGPVLLVTVV...  \n",
              "4     EQSPQFLSIQEGENLTVYCNSSSVFSSLQWYRQEPGEGPVLLVTVV...  \n",
              "...                                                 ...  \n",
              "1521  QSPQSMFIQEGEDVSMNCTSSSIFNTWLWYKQEPGEGPVLLIALYK...  \n",
              "1522  PQALSIQEGENATMNCSYKTSINNLQWYRQNSGRGLVHLILIRSNE...  \n",
              "1523  EQSPQFLSIQEGENLTVYCNSSSVFSSLQWYRQEPGEGPVLLVTVV...  \n",
              "1524  QQVKQNSPSLSVQEGRISILNCDYTNSMFDYFLWYKKYPAEGPTFL...  \n",
              "1525  EQSPQSLIVQEGKNLTINCTSSKTLYGLYWYKQKYGEGLIFLMMLQ...  \n",
              "\n",
              "[1526 rows x 3 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "complex_sequences = extract_sequences(X_val)\n",
        "print(\"Showing MHC, peptide and TCR alpha/beta sequences for each complex\")\n",
        "complex_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-umTw2e_sVD2",
        "outputId": "3ed395ce-c021-4bd7-c92c-d0941300d842"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETL'\n",
            "  '1526']]\n"
          ]
        }
      ],
      "source": [
        "MHC_list = np.array(complex_sequences[\"MHC\"], dtype=str)\n",
        "unique_mhc, counts_mhc = np.unique(MHC_list, return_counts=True)\n",
        "print(np.asarray((unique_mhc, counts_mhc)).T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3H9th0gsVGY",
        "outputId": "ea641ada-d56a-40f2-ea9c-f2c7311df405"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['FLYALALLL' '38']\n",
            " ['GILGFVFTL' '866']\n",
            " ['GLCTLVAML' '278']\n",
            " ['IMDQVPFSV' '6']\n",
            " ['KLQCVDLHV' '4']\n",
            " ['KTWGQYWQV' '7']\n",
            " ['KVAELVHFL' '5']\n",
            " ['KVLEYVIKV' '9']\n",
            " ['LLFGYPVYV' '55']\n",
            " ['MLDLQPETT' '16']\n",
            " ['NLVPMVATV' '172']\n",
            " ['RMFPNAPYL' '14']\n",
            " ['RTLNAWVKV' '33']\n",
            " ['SLFNTVATL' '5']\n",
            " ['SLLMWITQV' '5']\n",
            " ['YLLEMLWRL' '13']]\n"
          ]
        }
      ],
      "source": [
        "peptide_list = np.array(complex_sequences[\"peptide\"], dtype=str)\n",
        "unique_peptide, counts_peptide = np.unique(peptide_list, return_counts=True)\n",
        "print(np.asarray((unique_peptide, counts_peptide)).T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTZ5whpEu4Tz",
        "outputId": "5ebc07a5-2fa0-4fe0-acc3-4f9e519a7505"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['ALNIQEGKTATLTCNYTNYSPAYLQWYRQDPGRGPVFLLLIRENEKEKRKERLKVTFDTTLKQSLFHITASQPADSATYLCALDMGGGSQGNLIFGKGTKLSVKPGITQSPKYLFRKEGQNVTLSCEQNLNHDAMYWYRQVPGQGLRLIYYSHIVNDFQKGDIAEGYSVSREKKESFPLTVTSAQKNPTAFYLCASSIRAADTQYFGPGT'\n",
            "  '2']\n",
            " ['ALNIQEGKTATLTCNYTNYSPAYLQWYRQDPGRGPVFLLLIRENEKEKRKERLKVTFDTTLKQSLFHITASQPADSATYLCALDPDTDKLIFGTGTRLQVFPGITQSPKYLFRKEGQNVTLSCEQNLNHDAMYWYRQDPGQGLRLIYYSQIVNDFQKGDIAEGYSVSREKKESFPLTVTSAQKNPTAFYLCASSMRASVEQFFGPGT'\n",
            "  '2']\n",
            " ['ALNIQEGKTATLTCNYTNYSPAYLQWYRQDPGRGPVFLLLIRENEKEKRKERLKVTFDTTLKQSLFHITASQPADSATYLCALDPRGASKIIFGSGTRLSIRPDVTQTPRNRITKTGKRIMLECSQTKGHDRMYWYRQDPGLGLRLIYYSFDVKDINKGEISDGYSVSRQAQAKFSLSLESAIPNQTALYFCATSDTQGGGQPQHFGDGTR'\n",
            "  '1']\n",
            " ...\n",
            " ['VQEGEDFTTYCNSSTTLSNIQWYKQRPGGHPVFLIQLVKSGEVKKKRLTFQFGEAKKNSSLHITATQTTDVGTYFCAGSYGGSQGNLIFGKGTKLSVKPGITQSPKYLFRKEGQNVTLSCEQNLNHDAMYWYRQDPGQGLRLIYYSQIVNDFQKGDIAEGYSVSREKKESFPLTVTSAQKNPTAFYLCASSSRSHQPQHFGDGT'\n",
            "  '2']\n",
            " ['VQEGEDFTTYCNSSTTLSNIQWYKQRPGGHPVFLIQLVKSGEVKKKRLTFQFGEAKKNSSLHITATQTTDVGTYFCALGSGNTGKLIFGQGTTLQVKPVAQSPRYKITEKSQAVAFWCDPISGHATLYWYRQILGQGPELLVQFQDESVVDDSQLPKDRFSAERLKGVDSTLKIQPAELGDSAMYLCASSSGVYEQYFGPGT'\n",
            "  '1']\n",
            " ['VQEGEDFTTYCNSSTTLSNIQWYKQRPGGHPVFLIQLVKSGEVKKKRLTFQFGEAKKNSSLHITATQTTDVGTYFCAPANQAGTALIFGKGTTLSVAGVTQTPKFRVLKTGQSMTLLCAQDMNHEYMYWYRQDPGMGLRLIHYSVGEGTTAKGEVPDGYNVSRLKKQNFLLGLESAAPSQTSVYFCASSYHSNQPQHFGDGTR'\n",
            "  '1']]\n"
          ]
        }
      ],
      "source": [
        "tcr_list = np.array(complex_sequences[\"tcr\"], dtype=str)\n",
        "unique_tcr, counts_tcr = np.unique(tcr_list, return_counts=True)\n",
        "print(np.asarray((unique_tcr, counts_tcr)).T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hedTFJhsvvFR"
      },
      "source": [
        "## Train for MHC (one common sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LC0QibAEv01u"
      },
      "outputs": [],
      "source": [
        "# Load ESM-1b model\n",
        "model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
        "batch_converter = alphabet.get_batch_converter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "cFaa37TvwKku"
      },
      "outputs": [],
      "source": [
        "# Prepare data\n",
        "data = [\n",
        "    (\"protein1\", unique_mhc[0]),\n",
        "]\n",
        "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "\n",
        "# Extract per-residue representations (on CPU)\n",
        "with torch.no_grad():\n",
        "    results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
        "token_representations_mhc = results[\"representations\"][33]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGk1RjAYztBu"
      },
      "source": [
        "## Train for peptide (14 sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "t-ys3NJKzqXC"
      },
      "outputs": [],
      "source": [
        "# Prepare data\n",
        "data = []\n",
        "for i in range(len(unique_peptide)):\n",
        "  data.append((str(i), unique_peptide[i]))\n",
        "\n",
        "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "\n",
        "# Extract per-residue representations (on CPU)\n",
        "with torch.no_grad():\n",
        "    results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
        "token_representations_peptide = results[\"representations\"][33]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "GIsRKev1ow_a"
      },
      "outputs": [],
      "source": [
        "# Here do the same for thc\n",
        "data = []\n",
        "for i in range(len(tcr_list)):\n",
        "  data.append((str(i), tcr_list[i]))\n",
        "\n",
        "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "\n",
        "# Extract per-residue representations (on CPU)\n",
        "with torch.no_grad():\n",
        "    results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
        "token_representations_peptide = results[\"representations\"][33]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "xPJp7pSgow_b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(16, 16)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Use pickle to save the matrices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "esm_train_on_X.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "undefined.ipython.kernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
